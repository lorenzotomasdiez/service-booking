# AI & Machine Learning Infrastructure Platform
# BarberPro Day 10 - O10-001 Implementation
# Scalable ML infrastructure for real-time predictions and analytics

version: '3.8'

services:
  # ML Model Serving Gateway
  ml-model-gateway:
    image: barberpro/ml-gateway:latest
    ports:
      - "8080:8080"
      - "8081:8081"  # Admin/metrics
    environment:
      - ML_SERVING_MODE=production
      - MODEL_REGISTRY_URL=http://ml-model-registry:5000
      - PREDICTION_CACHE_TTL=300
      - MAX_CONCURRENT_PREDICTIONS=1000
      - MODEL_VERSIONING=enabled
      - A_B_TESTING=enabled
    networks:
      - ml-network
    depends_on:
      - ml-model-registry
      - ml-prediction-cache
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # ML Model Registry
  ml-model-registry:
    image: barberpro/model-registry:latest
    environment:
      - REGISTRY_DATABASE_URL=postgresql://ml_user:${ML_DB_PASSWORD}@ml-database:5432/model_registry
      - MODEL_STORAGE_BACKEND=s3
      - AWS_S3_BUCKET=barberpro-ml-models
      - MODEL_VERSIONING_STRATEGY=semantic
      - AUTO_MODEL_DEPLOYMENT=true
    volumes:
      - ml-models:/var/lib/models
    networks:
      - ml-network
    ports:
      - "5000:5000"
    depends_on:
      - ml-database

  # ML Training Infrastructure
  ml-training-cluster:
    image: barberpro/ml-training:latest
    environment:
      - TRAINING_MODE=distributed
      - GPU_ENABLED=false  # CPU-based for cost optimization
      - TRAINING_QUEUE=redis://ml-prediction-cache:6379/1
      - MODEL_OUTPUT_PATH=/models
      - HYPERPARAMETER_OPTIMIZATION=enabled
      - TRAINING_METRICS_BACKEND=mlflow
    volumes:
      - ml-models:/models
      - ml-training-data:/data
    networks:
      - ml-network
    depends_on:
      - ml-prediction-cache
      - ml-mlflow
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.labels.workload == training

  # ML Inference Engine
  ml-inference-engine:
    image: barberpro/ml-inference:latest
    environment:
      - INFERENCE_FRAMEWORK=scikit-learn,tensorflow,pytorch
      - BATCH_SIZE=32
      - MAX_QUEUE_SIZE=1000
      - PREDICTION_TIMEOUT=5000  # 5 seconds
      - ASYNC_PREDICTIONS=enabled
      - MONITORING_ENABLED=true
    networks:
      - ml-network
    depends_on:
      - ml-model-registry
      - ml-prediction-cache
    deploy:
      replicas: 4
      resources:
        limits:
          cpus: '1.5'
          memory: 3G

  # Real-time Feature Store
  ml-feature-store:
    image: barberpro/feature-store:latest
    environment:
      - FEATURE_DATABASE_URL=postgresql://ml_user:${ML_DB_PASSWORD}@ml-database:5432/feature_store
      - ONLINE_STORE=redis://ml-prediction-cache:6379/2
      - OFFLINE_STORE=postgresql
      - FEATURE_VERSIONING=enabled
      - REAL_TIME_FEATURES=enabled
    volumes:
      - ml-feature-data:/var/lib/features
    networks:
      - ml-network
    depends_on:
      - ml-database
      - ml-prediction-cache

  # ML Pipeline Orchestrator
  ml-airflow:
    image: apache/airflow:2.7.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:${AIRFLOW_DB_PASSWORD}@ml-database:5432/airflow
      - AIRFLOW__CELERY__RESULT_BACKEND=redis://ml-prediction-cache:6379/3
      - AIRFLOW__CELERY__BROKER_URL=redis://ml-prediction-cache:6379/3
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=false
      - AIRFLOW__CORE__LOAD_EXAMPLES=false
      - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=true
    volumes:
      - ./ml-pipelines/dags:/opt/airflow/dags
      - ./ml-pipelines/plugins:/opt/airflow/plugins
      - ./ml-pipelines/requirements.txt:/requirements.txt
    networks:
      - ml-network
    ports:
      - "8082:8080"
    depends_on:
      - ml-database
      - ml-prediction-cache
    command: >
      bash -c "pip install -r /requirements.txt && 
               airflow db init && 
               airflow users create --username admin --firstname Admin --lastname User --role Admin --email admin@barberpro.ai --password ${AIRFLOW_ADMIN_PASSWORD} &&
               airflow webserver & airflow scheduler"

  # ML Experiment Tracking (MLflow)
  ml-mlflow:
    image: barberpro/mlflow:latest
    environment:
      - MLFLOW_BACKEND_STORE_URI=postgresql://ml_user:${ML_DB_PASSWORD}@ml-database:5432/mlflow
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://barberpro-ml-artifacts
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    networks:
      - ml-network
    ports:
      - "5001:5000"
    depends_on:
      - ml-database

  # ML Prediction Cache (Redis)
  ml-prediction-cache:
    image: redis:7-alpine
    command: >
      redis-server
      --maxmemory 4gb
      --maxmemory-policy allkeys-lru
      --save 900 1
      --save 300 10
      --save 60 10000
    volumes:
      - ml-cache-data:/data
    networks:
      - ml-network
    deploy:
      resources:
        limits:
          memory: 4G

  # ML Database
  ml-database:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=ml_platform
      - POSTGRES_USER=ml_user
      - POSTGRES_PASSWORD=${ML_DB_PASSWORD}
      - POSTGRES_MULTIPLE_DATABASES=feature_store,model_registry,mlflow,airflow
    volumes:
      - ml-db-data:/var/lib/postgresql/data
      - ./ml-scripts/init-ml-db.sql:/docker-entrypoint-initdb.d/01-init.sql
    networks:
      - ml-network

  # ML Data Processing Engine
  ml-spark-master:
    image: barberpro/spark:latest
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=ml-spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8083
    networks:
      - ml-network
    ports:
      - "8083:8083"
      - "7077:7077"

  ml-spark-worker:
    image: barberpro/spark:latest
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://ml-spark-master:7077
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_CORES=2
    networks:
      - ml-network
    depends_on:
      - ml-spark-master
    deploy:
      replicas: 3

  # ML Monitoring and Drift Detection
  ml-monitoring:
    image: barberpro/ml-monitoring:latest
    environment:
      - DRIFT_DETECTION=enabled
      - DRIFT_THRESHOLD=0.15
      - MODEL_PERFORMANCE_TRACKING=enabled
      - ALERT_WEBHOOK=${ML_ALERT_WEBHOOK}
      - MONITORING_INTERVAL=300  # 5 minutes
    volumes:
      - ml-monitoring-data:/var/lib/monitoring
    networks:
      - ml-network
    depends_on:
      - ml-model-registry
      - ml-prediction-cache

  # Real-time Data Streaming
  ml-kafka:
    image: confluentinc/cp-kafka:latest
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=ml-zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://ml-kafka:29092,PLAINTEXT_HOST://localhost:9092
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
    networks:
      - ml-network
    depends_on:
      - ml-zookeeper
    ports:
      - "9092:9092"

  ml-zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    networks:
      - ml-network

  # ML A/B Testing Service
  ml-ab-testing:
    image: barberpro/ab-testing:latest
    environment:
      - EXPERIMENT_DATABASE_URL=postgresql://ml_user:${ML_DB_PASSWORD}@ml-database:5432/ab_testing
      - TRAFFIC_SPLIT_STRATEGY=weighted_random
      - STATISTICAL_SIGNIFICANCE=0.05
      - MINIMUM_SAMPLE_SIZE=1000
    networks:
      - ml-network
    depends_on:
      - ml-database
      - ml-model-registry

  # ML Data Quality Monitor
  ml-data-quality:
    image: barberpro/data-quality:latest
    environment:
      - QUALITY_CHECKS=schema,null_values,outliers,drift
      - ALERT_THRESHOLD=0.1
      - CHECK_INTERVAL=3600  # 1 hour
      - DATA_PROFILING=enabled
    volumes:
      - ml-quality-reports:/var/lib/quality-reports
    networks:
      - ml-network
    depends_on:
      - ml-database
      - ml-kafka

  # ML Feature Engineering Pipeline
  ml-feature-pipeline:
    image: barberpro/feature-pipeline:latest
    environment:
      - PIPELINE_MODE=streaming
      - SOURCE_TOPICS=user_behavior,bookings,payments
      - TARGET_FEATURE_STORE=ml-feature-store:5000
      - FEATURE_WINDOW_SIZE=1h,24h,7d
      - REAL_TIME_AGGREGATION=enabled
    networks:
      - ml-network
    depends_on:
      - ml-kafka
      - ml-feature-store

  # ML Model Performance Analyzer
  ml-performance-analyzer:
    image: barberpro/model-performance:latest
    environment:
      - PERFORMANCE_METRICS=accuracy,precision,recall,f1,auc,latency
      - PERFORMANCE_THRESHOLD=0.85
      - MODEL_COMPARISON=enabled
      - CHAMPION_CHALLENGER=enabled
    networks:
      - ml-network
    depends_on:
      - ml-model-registry
      - ml-prediction-cache

networks:
  ml-network:
    driver: overlay
    encrypted: true
    attachable: false
    ipam:
      config:
        - subnet: 10.2.0.0/16

volumes:
  ml-models:
    driver: local
  ml-training-data:
    driver: local
  ml-feature-data:
    driver: local
  ml-cache-data:
    driver: local
  ml-db-data:
    driver: local
  ml-monitoring-data:
    driver: local
  ml-quality-reports:
    driver: local

# ML Model Templates
configs:
  user-behavior-model:
    file: ./ml-models/user_behavior_prediction.pkl
  booking-recommendation-model:
    file: ./ml-models/booking_recommendation.pkl
  pricing-optimization-model:
    file: ./ml-models/pricing_optimization.pkl
  demand-forecasting-model:
    file: ./ml-models/demand_forecasting.pkl

# ML Pipeline Definitions
x-ml-pipelines: &ml-pipelines
  - name: user_behavior_analysis
    schedule: "0 */6 * * *"  # Every 6 hours
    features: [session_duration, page_views, booking_conversion]
    target: user_engagement_score
    
  - name: booking_demand_prediction
    schedule: "0 2 * * *"   # Daily at 2 AM
    features: [historical_bookings, seasonality, provider_availability]
    target: booking_demand
    
  - name: pricing_optimization
    schedule: "0 4 * * *"   # Daily at 4 AM
    features: [competitor_pricing, demand_level, provider_rating]
    target: optimal_price
    
  - name: provider_recommendation
    schedule: "0 */2 * * *" # Every 2 hours
    features: [user_preferences, provider_ratings, availability]
    target: recommendation_score